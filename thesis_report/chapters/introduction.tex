\chapter{Introduction}

Pretrained language models have yielded state-of-the-art performance on a myriad of tasks along with reducing the computational resources often required to train NLP models by providing an advanced starting point for practitioners. Yet, due to the sheer size of the support of these models, the learned probability distribution over natural language strings is difficult to analyze. The support of the distribution alone---the set of all possible strings that can be built using a specific vocabulary---is countably infinite. While a number of techniques have recently been proposed for analyzing the attributes of natural language that these models learn \cite{desintpro, alain2016understanding, pandit2021probing, koto2021discourse, belinkov2022probing}, it is still unclear what portions of the semantic space they learn (or fail) to represent.

One set of techniques used to find out more about the specific properties of language learned by these models is called probing. Probing is a method where supervised models are trained to predict linguistic properties from text. If a property is encoded in the text, the resulting accuracy of the aforementioned models is assumed to be high. Probing is applied relative to some baseline and we just \textit{assume} the probe found the property. This process raises the question whether the probe interpreted the text it was given or just learned the task itself. \cite{desintpro}

We propose an alternative method to probing language models by utilizing a known technique from natural language processing: the topic model. Topic models cluster documents from a corpus into topics and provide insight into what words occur together and how often they occur in the analyzed text, i.e. the semantic space. Language models are generally trained on vast and unstructured text corpora. Through this they learn key mechanics of natural language \cite{rytting2021leveraging}, s.t. they can be fine-tuned on a variety of NLP tasks. This requires the model to learn the hidden structures and connections between sentences and words in the training corpus. That way, the model will be able to apply the learned knowledge to different subtasks, e.g. prediction of news articles or movie reviews. We call that generalizing beyond the training data. The problem with generalizing is that we hardly know how to control what the language model generalizes and what not, apart from the training data \cite{MarasovicGradient2018NLP}. 

By analyzing the semantic space with topic models, we try to gain a better understanding of the language model and the assumptions it makes to generalize beyond the observed data, i.e. the inductive bias. The inductive bias is encoded in the language modeling algorithm and therefore difficult to analyze from these \textit{black boxes}. By sampling strings (also called documents) from a pretrained language model, we generate a corpus. This corpus should provide an unbiased representation of the information learned by the model. We then train topic models---using techniques such as Latent Dirichlet Allocation (LDA) \cite{blei2003latent}---to understand the distribution over topics that the language model captures. Finally, we compare those distributions across different corpora to better comprehend this inductive bias.

We assume that a language model learns the language and therefore the semantic space of the learning data. By comparing the topic distributions from learned data to data generated by the trained language model, we might discover discrepancies in the comparison metric. Those discrepancies ought to indicate that the prior assumption, meaning the inductive bias, skews the models topic distribution. For example, this could hint towards the fact that models favor learning generalizations over specific semantic spaces. By comparing different topic models \cite{blei2003latent, neuralLDA} on different language models \cite{gpt-2, transformer-xl}, we expect the resulting topic models to provide us with valuable information about the impact the inductive bias of our language models has on future predictions.

In this thesis, we employ a combination of techniques from \textit{Topic Modeling} and \textit{Pretrained Language Models}. The first part of this thesis consists of creating a baseline. We train a Language Model (GPT-2 \cite{gpt-2}) and compare the learned topic model to the topic model learned from the original dataset. We then evaluate and compare topic models from different text corpora with each other using our own designed metric. The second part of this thesis focuses on analyzing the topic probability distributions to get further insights into where the found discrepancies in our metric originated from, i.e.:
\begin{itemize}
    \item the language model architecture,
    \item the sampling technique for generating text from a language model or
    \item design choices of the topic model (e.g. tokenization, model architecture, etc.).
\end{itemize}
To this end, we additionally train a second language model (Transformer-XL \cite{transformer-xl}) and expand our topic model technique with the neural LDA \cite{neuralLDA}. Moreover, we compare the impact of different sampling techniques for language models (Top-P sampling \cite{holtzman2019curious} vs. Typical sampling \cite{meister2022typical}) and various topic sizes ($\{2, 3, 5, 10, 20, 50, 100\}$). We also study the influence of the dictionary, i.e. the set of unique words, that our topic model uses for learning the topic distributions. By adding bigrams and trigrams to the dictionary, we hope to provide the topic model with more information about related words such that it can learn the semantic space better. 

This work exploits the abilities of topic models by applying them to a task for which they have not yet been used. By means of a topic analysis, we gain a better understanding of the characteristics of language learned by today's language models, i.e. the inductive bias. A deeper understanding opens the way to new research, for example in the direction of language model architecture, the way they are trained and employed for various NLP tasks, and the application of topic models.
