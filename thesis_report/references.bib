% ------------------------------------
% Language model literature
% ------------------------------------
@article{gupta2010survey,
  title={A survey of text summarization extractive techniques},
  author={Gupta, Vishal and Lehal, Gurpreet Singh},
  journal={Journal of emerging technologies in web intelligence},
  volume={2},
  number={3},
  pages={258--268},
  year={2010},
  publisher={Citeseer}
}
@article{rytting2021leveraging,
  title={Leveraging the Inductive Bias of Large Language Models for Abstract Textual Reasoning},
  author={Rytting, Christopher and Wingate, David},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@article{bengio2003neural,
  title={A neural probabilistic language model},
  author={Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
  journal={Journal of machine learning research},
  volume={3},
  number={Feb},
  pages={1137--1155},
  year={2003}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{heilbron2019tracking,
  title={Tracking naturalistic linguistic predictions with deep neural language models},
  author={Heilbron, Micha and Ehinger, Benedikt and Hagoort, Peter and De Lange, Floris P},
  journal={arXiv preprint arXiv:1909.04400},
  year={2019}
}
@misc{radford2018improving,
  title={Improving language understanding by generative pre-training (2018)},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018}
}
@article{gpt-2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@article{gpt-3,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}
@article{transformer-xl,
  title={Transformer-xl: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}
@misc{attentionmechanism,
  title = {A Brief Overview of Attention Mechanism},
  howpublished = {\url{https://medium.com/syncedreview/a-brief-overview-of-attention-mechanism-13c578ba9129}},
  note = {Accessed: 2022-05-07}
}
@article{karami2018fuzzy,
  title={Fuzzy approach topic discovery in health and medical corpora},
  author={Karami, Amir and Gangopadhyay, Aryya and Zhou, Bin and Kharrazi, Hadi},
  journal={International Journal of Fuzzy Systems},
  volume={20},
  number={4},
  pages={1334--1345},
  year={2018},
  publisher={Springer}
}
@inproceedings{zhang2022pre,
  title={Pre-training and Fine-tuning Neural Topic Model: A Simple yet Effective Approach to Incorporating External Knowledge},
  author={Zhang, Linhai and Hu, Xuemeng and Wang, Boyu and Zhou, Deyu and Zhang, Qian-Wen and Cao, Yunbo},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={5980--5989},
  year={2022}
}
@article{MarasovicGradient2018NLP,
  author = {Marasović, Ana},
  title = {NLP’s generalization problem, and how researchers are tackling it},
  journal = {The Gradient},
  year = {2018},
  howpublished = {\url{https://thegradient.pub/frontiers-of-generalization-in-natural-language-processing}},
}
@article{kim2017structured,
  title={Structured attention networks},
  author={Kim, Yoon and Denton, Carl and Hoang, Luong and Rush, Alexander M},
  journal={arXiv preprint arXiv:1702.00887},
  year={2017}
}
@inproceedings{yang2016hierarchical,
  title={Hierarchical attention networks for document classification},
  author={Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
  booktitle={Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies},
  pages={1480--1489},
  year={2016}
}
@misc{feedforward,
  title = {When Recurrent Models Don't Need to be Recurrent},
  howpublished = {\url{https://bair.berkeley.edu/blog/2018/08/06/recurrent/}},
  note = {Accessed: 2022-05-07}
}
@misc{howdotrafo,
  title = {How do Transformers Work in NLP? A Guide to the Latest State-of-the-Art Models},
  howpublished = {\url{https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/}},
  note = {Accessed: 2022-05-07}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@misc{illustratedtransformer,
  title = {The Illustrated Transformer},
  howpublished = {\url{https://jalammar.github.io/illustrated-transformer/}},
  note = {Accessed: 2022-05-07}
}
@misc{illuaatte,
  title = {The Illustrated Transformer},
  howpublished = {\url{https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a}},
  note = {Accessed: 2022-05-07}
}
@misc{trafoarch,
  title = {Examining the Transformer Architecture},
  howpublished = {\url{https://towardsdatascience.com/examining-the-transformer-architecture-part-1-the-openai-gpt-2-controversy-feceda4363bb}},
  note = {Accessed: 2022-05-07}
}
@article{dowdell2020language,
  title={Language Modelling for Source Code with Transformer-XL},
  author={Dowdell, Thomas and Zhang, Hongyu},
  journal={arXiv preprint arXiv:2007.15813},
  year={2020}
}
@misc{trafogen,
  title = {How to generate text: using different decoding methods for language generation with Transformers},
  howpublished = {\url{https://huggingface.co/blog/how-to-generate}},
  note = {Accessed: 2022-05-07}
}
@article{meister2022typical,
  title={Typical decoding for natural language generation},
  author={Meister, Clara and Pimentel, Tiago and Wiher, Gian and Cotterell, Ryan},
  journal={arXiv preprint arXiv:2202.00666},
  year={2022}
}
@article{holtzman2019curious,
  title={The curious case of neural text degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09751},
  year={2019}
}
@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

% ------------------------------------
% Perplexity literature
% ------------------------------------
@misc{perplexity,
  title = {Perplexity in Language Models},
  howpublished = {\url{https://towardsdatascience.com/perplexity-in-language-models-87a196019a94}},
  note = {Accessed: 2022-05-07}
}

% ------------------------------------
% Probing literature
% ------------------------------------
@article{gul2019linspector,
  title={LINSPECTOR: Multilingual Probing Tasks for Word Representations},
  author={G{\"u}l {\c{S}}ahin, G{\"o}zde and Vania, Clara and Kuznetsov, Ilia and Gurevych, Iryna},
  journal={arXiv e-prints},
  pages={arXiv--1903},
  year={2019}
}
@misc{desintpro,
  title = {Designing and Interpreting Probes},
  howpublished = {\url{https://nlp.stanford.edu/~johnhew/interpreting-probes.html}},
  note = {Accessed: 2022-05-07}
}
@article{alain2016understanding,
  title={Understanding intermediate layers using linear classifier probes},
  author={Alain, Guillaume and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1610.01644},
  year={2016}
}
@article{pandit2021probing,
  title={Probing for bridging inference in transformer language models},
  author={Pandit, Onkar and Hou, Yufang},
  journal={arXiv preprint arXiv:2104.09400},
  year={2021}
}
@article{koto2021discourse,
  title={Discourse probing of pretrained language models},
  author={Koto, Fajri and Lau, Jey Han and Baldwin, Timothy},
  journal={arXiv preprint arXiv:2104.05882},
  year={2021}
}
@article{belinkov2022probing,
  title={Probing classifiers: Promises, shortcomings, and advances},
  author={Belinkov, Yonatan},
  journal={Computational Linguistics},
  volume={48},
  number={1},
  pages={207--219},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}
@inproceedings{hewitt2019structural,
  title={A structural probe for finding syntax in word representations},
  author={Hewitt, John and Manning, Christopher D},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4129--4138},
  year={2019}
}
@article{white2021non,
  title={A non-linear structural probe},
  author={White, Jennifer C and Pimentel, Tiago and Saphra, Naomi and Cotterell, Ryan},
  journal={arXiv preprint arXiv:2105.10185},
  year={2021}
}
@misc{lingwis,
  title = {Linguistics Wisdom of NLP Models},
  howpublished = {\url{https://towardsdatascience.com/linguistics-wisdom-of-nlp-models-8c8554bc8c66}},
  note = {Accessed: 2022-05-07}
}

% ------------------------------------
% Topic Model literature
% ------------------------------------
@article{blei2003latent,
  title={Latent dirichlet allocation},
  author={Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  journal={Journal of machine Learning research},
  volume={3},
  number={Jan},
  pages={993--1022},
  year={2003}
}
@article{neuralLDA,
  title={Autoencoding variational inference for topic models},
  author={Srivastava, Akash and Sutton, Charles},
  journal={arXiv preprint arXiv:1703.01488},
  year={2017}
}
@article{mackay1998choice,
  title={Choice of basis for Laplace approximation},
  author={MacKay, David JC},
  journal={Machine learning},
  volume={33},
  number={1},
  pages={77--86},
  year={1998},
  publisher={Springer}
}
@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}
@inproceedings{syed2017full,
  title={Full-text or abstract? Examining topic coherence scores using latent dirichlet allocation},
  author={Syed, Shaheen and Spruit, Marco},
  booktitle={2017 IEEE International conference on data science and advanced analytics (DSAA)},
  pages={165--174},
  year={2017},
  organization={IEEE}
}
@misc{topiccoherencemeasures,
  title = {Understanding Topic Coherence Measures},
  howpublished = {\url{https://towardsdatascience.com/understanding-topic-coherence-measures-4aa41339634c}},
  note = {Accessed: 2022-05-07}
}
@inproceedings{roder2015exploring,
  title={Exploring the space of topic coherence measures},
  author={R{\"o}der, Michael and Both, Andreas and Hinneburg, Alexander},
  booktitle={Proceedings of the eighth ACM international conference on Web search and data mining},
  pages={399--408},
  year={2015}
}
@inproceedings{fuglede2004jensen,
  title={Jensen-Shannon divergence and Hilbert space embedding},
  author={Fuglede, Bent and Topsoe, Flemming},
  booktitle={International Symposium onInformation Theory, 2004. ISIT 2004. Proceedings.},
  pages={31},
  year={2004},
  organization={IEEE}
}
@misc{tmint,
  title = {Topic Modeling: An Introduction},
  howpublished = {\url{https://monkeylearn.com/blog/introduction-to-topic-modeling/}},
  note = {Accessed: 2022-05-07}
}
@article{taylor2021variational,
  title={Variational message passing (VMP) applied to LDA},
  author={Taylor, Rebecca and Preez, Johan A du},
  journal={arXiv preprint arXiv:2111.01480},
  year={2021}
}
@article{gao2016streaming,
  title={Streaming Gibbs sampling for LDA model},
  author={Gao, Yang and Chen, Jianfei and Zhu, Jun},
  journal={arXiv preprint arXiv:1601.01142},
  year={2016}
}
@inproceedings{he2019end,
  title={An End-to-End Community Detection Model: Integrating LDA into Markov Random Field via Factor Graph.},
  author={He, Dongxiao and Song, Wenze and Jin, Di and Feng, Zhiyong and Huang, Yuxiao},
  booktitle={IJCAI},
  pages={5730--5736},
  year={2019}
}
@article{hoffman2010online,
  title={Online learning for latent dirichlet allocation},
  author={Hoffman, Matthew and Bach, Francis and Blei, David},
  journal={advances in neural information processing systems},
  volume={23},
  year={2010}
}
@misc{cotterell2021,
  author        = {Ryan Cotterell},
  title         = {Lecture notes in Natural Language Processing},
  month         = {February},
  year          = {2021},
  publisher={ETH Zürich}
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@misc{slp3,
  title = {Speech and Language Processing (3rd ed. draft)},
  author={Dan Jurafsky and James H. Martin},
  howpublished = {\url{https://web.stanford.edu/~jurafsky/slp3/}},
  note = {Accessed: 2022-05-07}
}

% ------------------------------------
% Problem statement literature
% ------------------------------------
@misc{aidungeon,
  title = {AI Dungeon},
  howpublished = {\url{https://play.aidungeon.io}},
  note = {Accessed: 2022-05-07}
}
@misc{racistai,
  title = {Microsoft's AI Twitter bot goes dark after racist, sexist tweets},
  howpublished = {\url{https://www.reuters.com/article/us-microsoft-twitter-bot-idUSKCN0WQ2LA}},
  note = {Accessed: 2022-05-07}
}
@misc{lmfail,
  title = {Language models fail to say what they mean or mean what they say},
  howpublished = {\url{https://venturebeat.com/2022/03/29/language-models-fail-to-say-what-they-mean-or-mean-what-they-say/}},
  note = {Accessed: 2022-05-07}
}
@inproceedings{abid2021persistent,
  title={Persistent anti-muslim bias in large language models},
  author={Abid, Abubakar and Farooqi, Maheen and Zou, James},
  booktitle={Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
  pages={298--306},
  year={2021}
}
@article{kim2019insider,
  title={Insider threat detection based on user behavior modeling and anomaly detection algorithms},
  author={Kim, Junhong and Park, Minsik and Kim, Haedong and Cho, Suhyoun and Kang, Pilsung},
  journal={Applied Sciences},
  volume={9},
  number={19},
  pages={4018},
  year={2019},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@misc{wikitext,
  title = {The WikiText Long Term Dependency Language Modeling Dataset},
  howpublished = {\url{https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/}},
  note = {Accessed: 2022-05-07}
}
@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}
@misc{arxiv,
  title = {arXiv Dataset},
  howpublished = {\url{https://www.kaggle.com/datasets/Cornell-University/arxiv}},
  note = {Accessed: 2022-05-07}
}
@misc{trafoxlmodel,
  title = {Original Transformer XL Model},
  howpublished = {\url{https://huggingface.co/transfo-xl-wt103}},
  note = {Accessed: 2022-05-07}
}
@misc{gpt2model,
  title = {Original GPT-2 Model},
  howpublished = {\url{https://huggingface.co/gpt2}},
  note = {Accessed: 2022-05-07}
}

% ------------------------------------
% Experimentation literature
% ------------------------------------
@misc{gitrepo,
  title = {Github Repository of this Thesis},
  howpublished = {\url{https://github.com/Frankie8472/masters-thesis}},
  note = {Accessed: 2022-05-07}
}
@inproceedings{huggingface,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}
@inproceedings{gensim,
      title = {{Software Framework for Topic Modelling with Large Corpora}},
      author = {Radim {\v R}eh{\r u}{\v r}ek and Petr Sojka},
      booktitle = {{Proceedings of the LREC 2010 Workshop on New
           Challenges for NLP Frameworks}},
      pages = {45--50},
      year = 2010,
      month = May,
      day = 22,
      publisher = {ELRA},
      address = {Valletta, Malta},
      note={\url{http://is.muni.cz/publication/884893/en}},
      language={English}
}
@misc{octis,
  title = {OCTIS : Optimizing and Comparing Topic Models is Simple!},
  howpublished = {\url{https://github.com/MIND-Lab/OCTIS}},
  note = {Accessed: 2022-05-07}
}
@misc{euler,
  title = {Euler},
  howpublished = {\url{https://scicomp.ethz.ch/wiki/Euler}},
  note = {Accessed: 2022-05-07}
}


% ------------------------------------
% Discussion literature
% ------------------------------------






