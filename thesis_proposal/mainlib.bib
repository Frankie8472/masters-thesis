@book{vapnik2013nature,
  title={The Nature of Statistical Learning Theory},
  author={Vapnik, Vladimir},
  year={2013},
  publisher={Springer Science \& Business Media}
}
@article{raffel2020,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}
@inproceedings{Dathathri2020Plug,
title={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},
author={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=H1edEyBKDS}
}
@phdthesis{hovy,
author = {Hovy, Eduard Hendrik},
title = {Generating Natural Language under Pragmatic Constraints},
year = {1987},
publisher = {Yale University},
address = {USA},
note = {AAI8729079}
}
@article{zador2019critique,
  title={A critique of pure learning and what artificial neural networks can learn from animal brains},
  author={Zador, Anthony M.},
  journal={Nature Communications},
  volume={10},
  number={1},
  pages={1--7},
  year={2019},
  publisher={Nature Publishing Group}
}
@article{radford2019language,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	language = {en},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	OPTpages = {24},
	year={2019},
	url = {https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf}
}
@article{merity2017regularizing,
    title={Regularizing and Optimizing LSTM Language Models},
    author={Stephen Merity and Nitish Shirish Keskar and Richard Socher},
    year={2017},
    eprint={1708.02182},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    journal   = {CoRR},
    volume    = {abs/1708.02182},
}
@article{holtzman2019curious,
  title={The Curious Case of Neural Text Degeneration},
  author={Holtzman, Ari and Buys, Jan and Forbes, Maxwell and Choi, Yejin},
  journal={Proceedings of the International Conference on Learning Representations},
  year={2020},
  url={http://arxiv.org/abs/1904.09751}
}
@misc{welleck2019neural,
    title={Neural Text Generation with Unlikelihood Training},
    author={Sean Welleck and Ilia Kulikov and Stephen Roller and Emily Dinan and Kyunghyun Cho and Jason Weston},
    year={2019},
    eprint={1908.04319},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@article{tacl2019,
    author = {Takahashi, Shuntaro and Tanaka-Ishii, Kumiko},
    title = "{Evaluating Computational Language Models with Scaling Properties of Natural Language}",
    journal = {Computational Linguistics},
    volume = {45},
    number = {3},
    pages = {481-513},
    year = {2019},
    month = {09},
    abstract = "{In this article, we evaluate computational models of natural language with respect to the universal statistical behaviors of natural language. Statistical mechanical analyses have revealed that natural language text is characterized by scaling properties, which quantify the global structure in the vocabulary population and the long memory of a text. We study whether five scaling properties (given by Zipf’s law, Heaps’ law, Ebeling’s method, Taylor’s law, and long-range correlation analysis) can serve for evaluation of computational models. Specifically, we test n-gram language models, a probabilistic context-free grammar, language models based on Simon/Pitman-Yor processes, neural language models, and generative adversarial networks for text generation. Our analysis reveals that language models based on recurrent neural networks with a gating mechanism (i.e., long short-term memory; a gated recurrent unit; and quasi-recurrent neural networks) are the only computational models that can reproduce the long memory behavior of natural language. Furthermore, through comparison with recently proposed model-based evaluation methods, we find that the exponent of Taylor’s law is a good indicator of model quality.}",
    issn = {0891-2017},
    doi = {10.1162/coli_a_00355},
    url = {https://doi.org/10.1162/coli\_a\_00355},
    eprint = {https://direct.mit.edu/coli/article-pdf/45/3/481/1847468/coli\_a\_00355.pdf},
}
@inproceedings{pascual+al.emnlp21,
 title = {A Plug-and-Play Method for Controlled Text Generation},
 author = {Pascual, Damian and 
Egressy, Beni and 
Meister, Clara and 
Cotterell, Ryan and 
Wattenhofer, Roger},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2021},
 month = {November},
 year = {2021},
 publisher = {Association for Computational Linguistics},
 url = {https://arxiv.org/abs/2109.09707},
 arxiv = {https://arxiv.org/abs/2109.09707},
 abstract = {Large pre-trained language models have repeatedly shown their ability to produce fluent text. Yet even when starting from a prompt, generation can continue in many plausible directions. Current decoding methods with the goal of controlling generation, e.g., to ensure specific words are included, either require additional models or fine-tuning, or work poorly when the task at hand is semantically unconstrained, e.g., story generation. In this work, we present a plug-and-play decoding method for controlled language generation that is so simple and intuitive, it can be described in a single sentence: given a topic or keyword, we add a shift to the probability distribution over our vocabulary towards semantically similar words. We show how annealing this distribution can be used to impose hard constraints on language generation, something no other plug-and-play method is currently able to do with SOTA language generators. Despite the simplicity of this approach, we see it works incredibly well in practice: decoding from GPT-2 leads to diverse and fluent sentences while guaranteeing the appearance of given guide words. We perform two user studies, revealing that (1) our method outperforms competing methods in human evaluations; and (2) forcing the guide words to appear in the generated text has no impact on the fluency of the generated text.}
}
@inproceedings{peters-etal-2019-sparse,
    title = "Sparse Sequence-to-Sequence Models",
    author = "Peters, Ben  and
      Niculae, Vlad  and
      Martins, Andr{\'e} F. T.",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1146",
    doi = "10.18653/v1/P19-1146",
    pages = "1504--1519",
    abstract = "Sequence-to-sequence models are a powerful workhorse of NLP. Most variants employ a softmax transformation in both their attention mechanism and output layer, leading to dense alignments and strictly positive output probabilities. This density is wasteful, making models less interpretable and assigning probability mass to many implausible outputs. In this paper, we propose sparse sequence-to-sequence models, rooted in a new family of $\alpha$-entmax transformations, which includes softmax and sparsemax as particular cases, and is sparse for any $\alpha > 1$. We provide fast algorithms to evaluate these transformations and their gradients, which scale well for large vocabulary sizes. Our models are able to produce sparse alignments and to assign nonzero probability to a short list of plausible outputs, sometimes rendering beam search exact. Experiments on morphological inflection and machine translation reveal consistent gains over dense models.",
}
@misc{bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{gpt2,
  added-at = {2019-02-27T03:35:25.000+0100},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  biburl = {https://www.bibsonomy.org/bibtex/2b30710316a8cfbae687672ea1f85c193/kirk86},
  description = {Language Models are Unsupervised Multitask Learners},
  interhash = {ce8168300081d74707849ed488e2a458},
  intrahash = {b30710316a8cfbae687672ea1f85c193},
  keywords = {learning multitask},
  timestamp = {2019-02-27T03:35:25.000+0100},
  title = {Language Models are Unsupervised Multitask Learners},
  url = {https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf},
  year = 2018
}
@misc{gpt3,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{lda,
      title={Latent Dirichlet Allocation (LDA) and Topic modeling: models, applications, a survey}, 
      author={Hamed Jelodar and Yongli Wang and Chi Yuan and Xia Feng and Xiahui Jiang and Yanchao Li and Liang Zhao},
      year={2018},
      eprint={1711.04305},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}
@misc{topicnl,
      title={Topic Compositional Neural Language Model}, 
      author={Wenlin Wang and Zhe Gan and Wenqi Wang and Dinghan Shen and Jiaji Huang and Wei Ping and Sanjeev Satheesh and Lawrence Carin},
      year={2018},
      eprint={1712.09783},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{survey,
      title={Topic Modelling Meets Deep Neural Networks: A Survey}, 
      author={He Zhao and Dinh Phung and Viet Huynh and Yuan Jin and Lan Du and Wray Buntine},
      year={2021},
      eprint={2103.00498},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{ptm2020,
   title={Pre-trained models for natural language processing: A survey},
   volume={63},
   ISSN={1869-1900},
   url={http://dx.doi.org/10.1007/s11431-020-1647-3},
   DOI={10.1007/s11431-020-1647-3},
   number={10},
   journal={Science China Technological Sciences},
   publisher={Springer Science and Business Media LLC},
   author={Qiu, XiPeng and Sun, TianXiang and Xu, YiGe and Shao, YunFan and Dai, Ning and Huang, XuanJing},
   year={2020},
   month={Sep},
   pages={1872–1897}
}
@misc{oppo,
      title={On the Opportunities and Risks of Foundation Models}, 
      author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and Shyamal Buch and Dallas Card and Rodrigo Castellon and Niladri Chatterji and Annie Chen and Kathleen Creel and Jared Quincy Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren Gillespie and Karan Goel and Noah Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and Omar Khattab and Pang Wei Koh and Mark Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Ben Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and Julian Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Rob Reich and Hongyu Ren and Frieda Rong and Yusuf Roohani and Camilo Ruiz and Jack Ryan and Christopher Ré and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishnan Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tramèr and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
      year={2021},
      eprint={2108.07258},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{topicaug,
author = {tang, hongyin and Li, Miao and Jin, Beihong},
year = {2019},
month = {01},
pages = {5093-5102},
title = {A Topic Augmented Text Generation Model: Joint Learning of Semantics and Structural Features},
doi = {10.18653/v1/D19-1513}
}
@misc{topicguided,
      title={Topic-Guided Variational Autoencoders for Text Generation}, 
      author={Wenlin Wang and Zhe Gan and Hongteng Xu and Ruiyi Zhang and Guoyin Wang and Dinghan Shen and Changyou Chen and Lawrence Carin},
      year={2019},
      eprint={1903.07137},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{topically,
      title={Topically Driven Neural Language Model}, 
      author={Jey Han Lau and Timothy Baldwin and Trevor Cohn},
      year={2017},
      eprint={1704.08012},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{interpol,
title = {On interpolating between probability distributions},
journal = {Applied Mathematics and Computation},
volume = {77},
number = {2},
pages = {213-244},
year = {1996},
issn = {0096-3003},
doi = {https://doi.org/10.1016/S0096-3003(95)00216-2},
url = {https://www.sciencedirect.com/science/article/pii/S0096300395002162},
author = {Faruk H. Bursal},
abstract = {Uncertainty in the simulation of physical systems is common and may be due to parameter variations or noise. When the uncertainty is random in nature, probability distributions for the quantities of interest are obtained. Sometimes, knowing only the mean and variance is sufficient; at other times, safety and reliability considerations require knowledge of the complete distribution. Computing these distributions is often a time-consuming task and needs to be repeated when some system parameters are changed. In this paper, formulas for interpolating between probability density and mass functions in spaces of arbitrary dimensionality are presented. It is found that these formulas give accurate results even when the functions one is interpolating between are not that “close”. As the mesh used in interpolation is refined, the accuracy of the interpolated quantities increases; accordingly, in addition to the more complicated and robust interpolation formulas meant for the case of a coarse mesh, simplified versions that result in good accuracy when the mesh is fine are also given. Savings in computational effort up to a factor of one hundred are common even when the more complicated interpolation formulas are used. This means that interpolation is a lucrative alternative to Monte Carlo simulation and even to the Generalized Cell Mapping (GCM) method when complete probability distributions, as opposed to only the low-order statistics, are needed. It is expected that this technique will relieve much of the burden of repeated, time-consuming simulations as certain relevant parameters are varied. In addition, the given formulas may be interpreted as general-purpose algorithms for the blending of shapes, thereby leading to applications beyond what is considered in this paper.}
}