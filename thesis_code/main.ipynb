{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Analysing pre-trained language models with topic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import set_seed\n",
    "from transformers import OpenAIGPTLMHeadModel, GPT2LMHeadModel, GPTNeoForCausalLM, TransfoXLLMHeadModel\n",
    "from transformers import OpenAIGPTTokenizer, GPT2Tokenizer, TransfoXLTokenizer\n",
    "from gensim.corpora.textcorpus import TextCorpus\n",
    "\n",
    "# Seed for reproducability\n",
    "set_seed(42)\n",
    "\n",
    "# Tensorflow or Pytorch\n",
    "platform = \"pt\"     # \"tf\" but not configured for that\n",
    "\n",
    "# Use GPU or CPU\n",
    "use_gpu = False\n",
    "torch.set_num_threads(torch.get_num_threads()*2-1)\n",
    "device = \"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\"\n",
    "\n",
    "# params\n",
    "corpus_size = 1e6   # depending on the number of topics created with lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Generating documents from selected pre-trained language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def create_corpus(\n",
    "        corpus_size=1,\n",
    "        model_name=\"openai-gpt\",\n",
    "        max_document_length=None,\n",
    "        tokenizer_model=AutoTokenizer,\n",
    "        lm_model=AutoModelForCausalLM,\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "    r\"\"\"\n",
    "    Generates sequences/documents/a corpus for models with a language modeling head.\n",
    "\n",
    "    Parameters:\n",
    "        corpus_size (`int`, *optional*, defaults to 1):\n",
    "            The corpus size to be generated (number of documents)\n",
    "        model_name (`str`, *optional*, defaults to \"openai-gpt\"):\n",
    "            The model name of the pre-trained model: openai-gpt, gpt2-small, gpt2, gpt2-large, gpt2-xl, transfo-xl-wt103, EleutherAI/gpt-neo-2.7B, ctrl\n",
    "        max_document_length (`int`, *optional*, defaults to None):\n",
    "            The maximum document length, normally set to tokenizer.max_length\n",
    "        tokenizer_model (`PreTrainedTokenizer`, *optional*, defaults to AutoTokenizer):\n",
    "            The pre-trained tokenizer class\n",
    "        lm_model (`PreTrainedModel`, *optional*, defaults to AutoModelForCausalLM):\n",
    "            The pre-trained model class with language modeling head\n",
    "        device (`str`, *optional*, defaults to \"cpu\"):\n",
    "            The device the computations commence \"cpu\" or \"cuda\"\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = tokenizer_model.from_pretrained(model_name)\n",
    "    model = lm_model.from_pretrained(model_name)\n",
    "\n",
    "    max_document_length = max_document_length if max_document_length is not None else tokenizer.model_max_length\n",
    "\n",
    "    print(f\"EOS: {tokenizer.eos_token} | BOS: {tokenizer.bos_token} | UNK: {tokenizer.unk_token}\")\n",
    "    eos_token_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.unk_token_id\n",
    "    bos_token_id = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else eos_token_id\n",
    "\n",
    "    model = model.to(device)\n",
    "    encoded_output = model.generate(\n",
    "        # all parameters have to be set as otherwise the config of the pretrained model will be taken\n",
    "        max_length=max_document_length,\n",
    "        do_sample=True,\n",
    "        early_stopping=False,\n",
    "        num_beams=1,                            # 1 deactivates beam_search\n",
    "        temperature=1.0,                        # 1.0 deactivates temperature\n",
    "        top_k=0,                                # 0 deactivates top_k sampling\n",
    "        top_p=1.0,                              # 1.0 deactivates top_p sampling\n",
    "        repetition_penalty=1.0,                 # 1.0 deactivates repetition_penalty\n",
    "        pad_token_id=eos_token_id,              # For open-end generation set to eos_token_id\n",
    "        bos_token_id=bos_token_id,\n",
    "        eos_token_id=eos_token_id,\n",
    "        length_penalty=1.0,                     # 1.0 deactivates length_penalty\n",
    "        no_repeat_ngram_size=0,                 # 0 deactivates no_repeat_ngram_size\n",
    "        encoder_no_repeat_ngram_size=0,         # 0 deactivates encoder_no_repeat_ngram_size\n",
    "        num_return_sequences=corpus_size,       # The number of independently computed returned sequences for each element in the batch. No input means batch size of one.\n",
    "        num_beam_groups=1,\n",
    "        output_scores=False,                    # Will be important if you want the prediction scores!\n",
    "    )\n",
    "    decoded_output = tokenizer.decode(encoded_output[0], skip_special_tokens=True)\n",
    "    print(decoded_output)\n",
    "    #TextCorpus.save_corpus(\"./data/corpus-\"+model_name)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of OpenAIGPTLMHeadModel were not initialized from the model checkpoint at openai-gpt and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using eos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS: None | BOS: None | UNK: <unk>\n",
      "hm. priests stole our thirteen - year -\n"
     ]
    }
   ],
   "source": [
    "create_corpus(1, \"openai-gpt\", 10, OpenAIGPTTokenizer, OpenAIGPTLMHeadModel, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS: <|endoftext|> | BOS: <|endoftext|> | UNK: <|endoftext|>\n",
      "Bapa Pinata and Momages Paranormal Journal (February 2018)\n",
      "\n",
      "no more scaling up construction and red flag (November 2018)\n",
      "\n",
      "sanctioned readmission of redacted website of the National Policy Group Working Group moderated last month\n",
      "\n",
      "an online violence website quietly downgraded to a disappeared link to the new shoddy work of the NGP and its 4th re.4 project (;)\n"
     ]
    }
   ],
   "source": [
    "create_corpus(1, \"gpt2\", 100, GPT2Tokenizer, GPT2LMHeadModel, device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS: <|endoftext|> | BOS: <|endoftext|> | UNK: <|endoftext|>\n",
      "Producer Lexus\" less\n",
      "\n",
      "Cable Network Specials\n",
      "\n",
      "After several months spent in the company of game shows like \"The New Price Is Right\" and fellow \"This Old House\" star Barbara Mandrell, \"Desperate Housewives\" Emmy-Award winner Felicity Huffman could be heard throughout \"Carpool Karaoke\" orchestrating a high-stake showdown for her home life. Top \"Modern Family\" actor Jesse Tyler Ferguson blended in one lucky week and\n"
     ]
    }
   ],
   "source": [
    "create_corpus(1, \"EleutherAI/gpt-neo-2.7B\", 100, GPT2Tokenizer, GPTNeoForCausalLM, device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS: <eos> | BOS: None | UNK: <unk>\n",
      "for reasons which include continued enthusiasm for the medium; frustration and frustration in Bernard discard all knowledge that the medium is capable of producing 32 million streams or fizzing advertisements for expensive medium.\n"
     ]
    }
   ],
   "source": [
    "create_corpus(1, \"transfo-xl-wt103\", 100, TransfoXLTokenizer, TransfoXLLMHeadModel, device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}