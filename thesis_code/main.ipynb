{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Analysing pre-trained language models with topic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch\n",
    "from transformers import set_seed\n",
    "from transformers import OpenAIGPTLMHeadModel, GPT2LMHeadModel, GPTNeoForCausalLM, TransfoXLLMHeadModel\n",
    "from transformers import OpenAIGPTTokenizer, GPT2Tokenizer, TransfoXLTokenizer\n",
    "from gensim.corpora.textcorpus import TextCorpus\n",
    "\n",
    "# Seed for reproducability\n",
    "set_seed(42)\n",
    "\n",
    "# Tensorflow or Pytorch\n",
    "platform = \"pt\"     # \"tf\" but not configured for that\n",
    "\n",
    "# Use GPU or CPU\n",
    "use_gpu = False\n",
    "torch.set_num_threads(torch.get_num_threads()*2-1)\n",
    "device = \"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\"\n",
    "\n",
    "# params\n",
    "corpus_size = 1e6   # depending on the number of topics created with lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Generating documents from selected pre-trained language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def create_corpus(\n",
    "        corpus_size=1,\n",
    "        model_name=\"openai-gpt\",\n",
    "        max_document_length=None,\n",
    "        tokenizer_model=AutoTokenizer,\n",
    "        lm_model=AutoModelForCausalLM,\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "    r\"\"\"\n",
    "    Generates sequences/documents/a corpus for models with a language modeling head.\n",
    "\n",
    "    Parameters:\n",
    "        corpus_size (`int`, *optional*, defaults to 1):\n",
    "            The corpus size to be generated (number of documents)\n",
    "        model_name (`str`, *optional*, defaults to \"openai-gpt\"):\n",
    "            The model name of the pre-trained model: openai-gpt, gpt2-small, gpt2, gpt2-large, gpt2-xl, transfo-xl-wt103, EleutherAI/gpt-neo-2.7B, ctrl\n",
    "        max_document_length (`int`, *optional*, defaults to None):\n",
    "            The maximum document length, normally set to tokenizer.max_length\n",
    "        tokenizer_model (`PreTrainedTokenizer`, *optional*, defaults to AutoTokenizer):\n",
    "            The pre-trained tokenizer class\n",
    "        lm_model (`PreTrainedModel`, *optional*, defaults to AutoModelForCausalLM):\n",
    "            The pre-trained model class with language modeling head\n",
    "        device (`str`, *optional*, defaults to \"cpu\"):\n",
    "            The device the computations commence \"cpu\" or \"cuda\"\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = tokenizer_model.from_pretrained(model_name)\n",
    "    model = lm_model.from_pretrained(model_name)\n",
    "\n",
    "    max_document_length = max_document_length if max_document_length is not None else tokenizer.model_max_length\n",
    "\n",
    "    print(f\"EOS: {tokenizer.eos_token} | BOS: {tokenizer.bos_token} | UNK: {tokenizer.unk_token}\")\n",
    "    eos_token_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.unk_token_id\n",
    "    bos_token_id = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else eos_token_id\n",
    "\n",
    "    model = model.to(device)\n",
    "    encoded_output = model.generate(\n",
    "        # all parameters have to be set as otherwise the config of the pretrained model will be taken\n",
    "        max_length=max_document_length,\n",
    "        do_sample=True,\n",
    "        early_stopping=False,\n",
    "        num_beams=1,                            # 1 deactivates beam_search\n",
    "        temperature=1.0,                        # 1.0 deactivates temperature\n",
    "        top_k=0,                                # 0 deactivates top_k sampling\n",
    "        top_p=1.0,                              # 1.0 deactivates top_p sampling\n",
    "        repetition_penalty=1.0,                 # 1.0 deactivates repetition_penalty\n",
    "        pad_token_id=eos_token_id,              # For open-end generation set to eos_token_id\n",
    "        bos_token_id=bos_token_id,\n",
    "        eos_token_id=eos_token_id,\n",
    "        length_penalty=1.0,                     # 1.0 deactivates length_penalty\n",
    "        no_repeat_ngram_size=0,                 # 0 deactivates no_repeat_ngram_size\n",
    "        encoder_no_repeat_ngram_size=0,         # 0 deactivates encoder_no_repeat_ngram_size\n",
    "        num_return_sequences=corpus_size,       # The number of independently computed returned sequences for each element in the batch. No input means batch size of one.\n",
    "        num_beam_groups=1,\n",
    "        output_scores=False,                    # Will be important if you want the prediction scores!\n",
    "    )\n",
    "    decoded_output = tokenizer.decode(encoded_output[0], skip_special_tokens=True)\n",
    "    print(decoded_output)\n",
    "    gc.collect()\n",
    "    #TextCorpus.save_corpus(\"./data/corpus-\"+model_name)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of OpenAIGPTLMHeadModel were not initialized from the model checkpoint at openai-gpt and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using eos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS: None | BOS: None | UNK: <unk>\n",
      "* \n",
      " four weeks, later \n",
      " \" do you think that your answer to my proposal would have been the same mrs. fredrick? \" fanny asked in a tone that suggested to phil that she had fun doing such things. they sat near noon at maple terrace's famed dance floor, a large banquet table with a large sprinkling of breads, ripping off smaller chunks of a certain bosom, and passing it back and forth against the lap of a winner, not everyone. phil and julia giggled at the scene\n"
     ]
    }
   ],
   "source": [
    "create_corpus(1, \"openai-gpt\", 100, OpenAIGPTTokenizer, OpenAIGPTLMHeadModel, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS: <|endoftext|> | BOS: <|endoftext|> | UNK: <|endoftext|>\n",
      "University of Iowa President, Anne Gust, was profiled in the January 2015 issue of The New Yorker.\n",
      "\n",
      "\"It is a superpower full of ambitions and wounds. And at its heart stands a monument to human folly.\"\n",
      "\n",
      "For the reasons Taylor Swift's \"Blank Space\" makes me laugh, Iowa, my home state, is one of the smartest places to live in America. But maybe most importantly, it's where tomorrow's industrial winners will emergeâ€”and where they'll get\n"
     ]
    }
   ],
   "source": [
    "create_corpus(1, \"gpt2-xl\", 100, GPT2Tokenizer, GPT2LMHeadModel, device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS: <|endoftext|> | BOS: <|endoftext|> | UNK: <|endoftext|>\n",
      "1. Field of the Invention\n",
      "The present invention relates to a memory element such as a memory device, an absolute type type memory having an absolute difference of the potential of a floating gate, and an analog type memory having a source potential arbitrarily controlled in a dynamic random access memory (DRAM: Dynamic Random Access Memory), for example. More particularly, the present invention relates to a combination of the memory element (memory cell) having the source potential controlled in the DRAM, and the memory element\n"
     ]
    }
   ],
   "source": [
    "create_corpus(1, \"EleutherAI/gpt-neo-2.7B\", 100, GPT2Tokenizer, GPTNeoForCausalLM, device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS: <eos> | BOS: None | UNK: <unk>\n",
      "Schmidt resigned as president of his own party.\n"
     ]
    }
   ],
   "source": [
    "create_corpus(1, \"transfo-xl-wt103\", 100, TransfoXLTokenizer, TransfoXLLMHeadModel, device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}