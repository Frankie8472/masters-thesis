{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Analysing pre-trained language models with topic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import random\n",
    "import numpy as np\n",
    "import os.path\n",
    "import logging\n",
    "import random\n",
    "import itertools\n",
    "from gensim.test.utils import datapath\n",
    "import re\n",
    "import json\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter, MaxNLocator\n",
    "from matplotlib.patches import Rectangle\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from pprint import pprint\n",
    "from gensim.utils import SaveLoad\n",
    "from gensim.corpora import Dictionary\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import set_seed\n",
    "from transformers import AutoModelForCausalLM, GPT2LMHeadModel, GPTNeoForCausalLM\n",
    "from transformers import AutoTokenizer, GPT2Tokenizer\n",
    "from gensim.models import Phrases\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Seed for reproducability\n",
    "set_seed(42)\n",
    "\n",
    "# Tensorflow or Pytorch\n",
    "framework = \"pt\"     # \"tf\" but not configured for that\n",
    "\n",
    "# Use GPU or CPU\n",
    "use_gpu = True\n",
    "torch.set_num_threads(torch.get_num_threads()*2-2)\n",
    "device = \"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\"\n",
    "\n",
    "# params\n",
    "corpus_size = 1e6   # depending on the number of topics created with lda\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experimenting individual functions on small data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Removing Wikitext 103 Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def removing_wikitext_titles(file_name):\n",
    "    r\"\"\"\n",
    "    Removing Titles and replacing them with one empty line between each text\n",
    "\n",
    "    :param file_name: location of the file with titles\n",
    "    \"\"\"\n",
    "    new_file_name = file_name+'_no_titles.txt'\n",
    "    if not os.path.isfile(new_file_name):\n",
    "        str0 = Path(file_name).read_text(encoding='utf-8')\n",
    "        heading_pattern = '( \\n [=\\s].*[=\\s] \\n)'\n",
    "        str1 = re.sub(heading_pattern, '\\n\\n', str0)\n",
    "        str2 = re.sub('\\n\\n+[\\n]', '\\n\\n', str1)\n",
    "        with open(file=new_file_name, mode='x', encoding='utf-8') as file:\n",
    "            file.write(str2)\n",
    "    else:\n",
    "        print(\"ERROR: File already exists. Must be deleted manually.\")\n",
    "\n",
    "train = './data/data_wikitext-103-raw/wiki.train.raw'\n",
    "test = './data/data_wikitext-103-raw/wiki.test.raw'\n",
    "valid = './data/data_wikitext-103-raw/wiki.valid.raw'\n",
    "removing_wikitext_titles(train)\n",
    "removing_wikitext_titles(test)\n",
    "removing_wikitext_titles(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Training your own language model\n",
    "\n",
    "`python run_clm.py --model_type gpt2 --tokenizer_name gpt2 --dataset_name wikitext --dataset_config_name wikitext-103-raw-v1 --output_dir data/test --do_eval --do_train --block_size 1024 --overwrite_output_dir`\n",
    "\n",
    "or\n",
    "\n",
    "`python run_clm.py --model_type gpt2 --tokenizer_name gpt2 --output_dir data/test --do_eval --do_train --block_size 1024 --overwrite_output_dir --train_file .\\data\\data_wikitext-103-raw\\wiki.train.raw.txt --validation_file .\\data\\data_wikitext-103-raw\\wiki.valid.raw.txt`\n",
    "\n",
    "**For validation/perplexity score only:**\n",
    "`python run_clm.py --model_type gpt2 --tokenizer_name gpt2 --output_dir data/test --do_eval --block_size 1024 --overwrite_output_dir --validation_file .\\data\\data_wikitext-103-raw\\wiki.valid.raw.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Generating documents from selected pre-trained language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:40: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:40: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "C:\\Users\\franz\\AppData\\Local\\Temp/ipykernel_32996/2422776061.py:40: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if pad_token_id is 'eos_token_id':\n"
     ]
    }
   ],
   "source": [
    "def create_corpus(\n",
    "        tokenizer_name=\"gpt2\",\n",
    "        model_name=\"gpt2\",\n",
    "        max_document_length=None,\n",
    "        device=\"cpu\",\n",
    "        corpus_size=1,\n",
    "        tokenizer_model=AutoTokenizer,\n",
    "        lm_model=AutoModelForCausalLM,\n",
    "        pad_token_id=None,\n",
    "        save_path=\"data/test\",\n",
    "        load_size=1\n",
    "    ):\n",
    "    r\"\"\"\n",
    "    Generates sequences/documents/a corpus for models with a language modeling head.\n",
    "\n",
    "    Parameters:\n",
    "        corpus_size (`int`, *optional*, defaults to 1):\n",
    "            The corpus size to be generated (number of documents)\n",
    "        model_name (`str`, *optional*, defaults to \"openai-gpt\"):\n",
    "            The model name of the pre-trained model: openai-gpt, gpt2-small, gpt2, gpt2-large, gpt2-xl, transfo-xl-wt103, EleutherAI/gpt-neo-2.7B, ctrl\n",
    "        max_document_length (`int`, *optional*, defaults to None):\n",
    "            The maximum document length, normally set to tokenizer.max_length\n",
    "        tokenizer_model (`PreTrainedTokenizer`, *optional*, defaults to AutoTokenizer):\n",
    "            The pre-trained tokenizer class\n",
    "        lm_model (`PreTrainedModel`, *optional*, defaults to AutoModelForCausalLM):\n",
    "            The pre-trained model class with language modeling head\n",
    "        device (`str`, *optional*, defaults to \"cpu\"):\n",
    "            The device the computations commence \"cpu\" or \"cuda\"\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.isfile(save_path):\n",
    "        print(\"ERROR: file already exist, please remove manually before running again.\")\n",
    "        return\n",
    "\n",
    "    tokenizer = tokenizer_model.from_pretrained(tokenizer_name)\n",
    "    model = lm_model.from_pretrained(model_name)\n",
    "\n",
    "    max_document_length = max_document_length if max_document_length is not None else tokenizer.model_max_length\n",
    "    if pad_token_id is not None:\n",
    "        if pad_token_id == 'eos_token_id':\n",
    "            pad_token_id = tokenizer.eos_token_id\n",
    "        else:\n",
    "            print(\"ERROR: Undefinded/unimplemented pad_token_id\")\n",
    "\n",
    "    # print(f\"EOS: {tokenizer.eos_token} | BOS: {tokenizer.bos_token} | UNK: {tokenizer.unk_token}\")\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    decoded_output = []\n",
    "\n",
    "    for i in tqdm(range(0, corpus_size, load_size)):\n",
    "        step_size = min(load_size, corpus_size-i)\n",
    "        encoded_output = model.generate(\n",
    "            # all parameters have to be set as otherwise the config of the pretrained model will be taken\n",
    "            input_ids=None,\n",
    "            max_length=max_document_length,\n",
    "            do_sample=True,                         # False implies Greedy search\n",
    "            early_stopping=False,\n",
    "            num_beams=1,                            # 1 deactivates beam_search\n",
    "            temperature=1.0,                        # 1.0 deactivates temperature\n",
    "            top_k=0,                                # 0 deactivates top_k sampling\n",
    "            top_p=1.0,                              # 1.0 deactivates top_p sampling\n",
    "            repetition_penalty=1.0,                 # 1.0 deactivates repetition_penalty\n",
    "            pad_token_id=pad_token_id,              # For open-end generation set to eos_token_id\n",
    "            #bos_token_id=bos_token_id,\n",
    "            #eos_token_id=eos_token_id,\n",
    "            length_penalty=1.0,                     # 1.0 deactivates length_penalty\n",
    "            no_repeat_ngram_size=0,                 # 0 deactivates no_repeat_ngram_size\n",
    "            encoder_no_repeat_ngram_size=0,         # 0 deactivates encoder_no_repeat_ngram_size\n",
    "            num_return_sequences=step_size,       # The number of independently computed returned sequences for each element in the batch. No input means batch size of one.\n",
    "            num_beam_groups=1,\n",
    "            output_scores=False,                    # Will be important if you want the prediction scores!\n",
    "        )\n",
    "\n",
    "        for j in range(load_size):\n",
    "            decoded_output.append(tokenizer.decode(encoded_output[j], skip_special_tokens=True))\n",
    "\n",
    "    with open(save_path, 'w') as file:\n",
    "        json.dump(decoded_output, file, indent=2)\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: file already exist, please remove manually before running again.\n"
     ]
    }
   ],
   "source": [
    "create_corpus(\n",
    "    tokenizer_name=\"gpt2\",\n",
    "    model_name=\"./data/model-gpt2-wiki_nt\",\n",
    "    max_document_length=None,\n",
    "    device=device,\n",
    "    corpus_size=1000,\n",
    "    tokenizer_model=GPT2Tokenizer,\n",
    "    lm_model=GPT2LMHeadModel,\n",
    "    pad_token_id='eos_token_id',\n",
    "    save_path=\"data/test.json\",\n",
    "    load_size=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78736e13350f4f59b0573c84c63df7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_corpus(\n",
    "    tokenizer_name=\"gpt2\",  # \"EleutherAI/gpt-neo-2.7B\"\n",
    "    model_name=\"gpt2\",  # \"EleutherAI/gpt-neo-2.7B\"\n",
    "    max_document_length=None,   # 2048\n",
    "    device=device,\n",
    "    corpus_size=1000,\n",
    "    tokenizer_model=GPT2Tokenizer,\n",
    "    lm_model=GPT2LMHeadModel,   # GPTNeoForCausalLM\n",
    "    pad_token_id='eos_token_id',\n",
    "    save_path=\"data/data_gpt2.json\",\n",
    "    load_size=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an LDA Model (Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.DEBUG)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def load_json(filename, samples=100000):\n",
    "    with open(filename, 'r') as file:\n",
    "        train_articles = json.load(file)\n",
    "    return random.sample(train_articles, k=samples)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def load_arxiv(samples=100000):\n",
    "    def get_metadata():\n",
    "        with open('data/data_arxiv-metadata-oai-snapshot.json', 'r') as f:\n",
    "            for line in f:\n",
    "                yield line\n",
    "\n",
    "    metadata = get_metadata()\n",
    "    size = 0\n",
    "    for paper in metadata:\n",
    "        size += 1\n",
    "    choices = random.sample(list(np.arange(size)), k=samples)\n",
    "    choices.sort()\n",
    "    metadata = get_metadata()\n",
    "    step = 0\n",
    "    idx = 0\n",
    "    corpus = []\n",
    "    for paper in metadata:\n",
    "        if idx >= samples:\n",
    "            break\n",
    "        if step == choices[idx]:\n",
    "            idx += 1\n",
    "            corpus.append(json.loads(paper)['abstract'])\n",
    "        step += 1\n",
    "    return corpus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:2: DeprecationWarning: invalid escape sequence \\s\n",
      "C:\\Users\\franz\\AppData\\Local\\Temp/ipykernel_30156/2425308730.py:2: DeprecationWarning: invalid escape sequence \\s\n",
      "  heading_pattern = '( \\n [=\\s].*[=\\s] \\n)'\n"
     ]
    }
   ],
   "source": [
    "def load_wikitext(samples=100000):\n",
    "    heading_pattern = '( \\n [=\\s].*[=\\s] \\n)'\n",
    "    train_data = Path('data/data_wikitext-103-raw/wiki.train.raw').read_text(encoding='utf-8')\n",
    "    train_split = re.split(heading_pattern, train_data)\n",
    "    train_headings = [x[7:-7] for x in train_split[1::2]]\n",
    "    train_articles = [x for x in train_split[2::2]]\n",
    "    return random.sample(train_articles, k=samples)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def tokenize(docs):\n",
    "    # Tokenize the documents.\n",
    "    # Split the documents into tokens.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for idx in range(len(docs)):\n",
    "        docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "    # Remove words that are only one character.\n",
    "    docs = [[token for token in doc if len(token) > 1] for doc in docs]\n",
    "\n",
    "    # Lemmatize the documents. Better than stemmer as is easier to read\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "\n",
    "    # Add bigrams to docs (only ones that appear 20 times or more).\n",
    "    bigram = Phrases(docs, min_count=20)\n",
    "    for idx in range(len(docs)):\n",
    "        for token in bigram[docs[idx]]:\n",
    "            if '_' in token:\n",
    "                # Token is a bigram, add to document.\n",
    "                docs[idx].append(token)\n",
    "\n",
    "    # Remove rare and common tokens.\n",
    "    # Create a dictionary representation of the documents.\n",
    "    dictionary = Dictionary(docs)\n",
    "\n",
    "    # Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "    dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "    # Bag-of-words representation of the documents.\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    print('Number of unique tokens: %d' % len(dictionary))\n",
    "    print('Number of documents: %d' % len(corpus))\n",
    "\n",
    "    return dictionary, corpus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_special(docs0, docs1, union=False):  # False is intersection\n",
    "    # Tokenize the documents.\n",
    "    # Split the documents into tokens.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    for idx in range(len(docs0)):\n",
    "        docs0[idx] = docs0[idx].lower()  # Convert to lowercase.\n",
    "        docs0[idx] = tokenizer.tokenize(docs0[idx])  # Split into words.\n",
    "    for idx in range(len(docs1)):\n",
    "        docs1[idx] = docs1[idx].lower()  # Convert to lowercase.\n",
    "        docs1[idx] = tokenizer.tokenize(docs1[idx])  # Split into words.\n",
    "\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    docs0 = [[token for token in doc if not token.isnumeric()] for doc in docs0]\n",
    "    docs1 = [[token for token in doc if not token.isnumeric()] for doc in docs1]\n",
    "\n",
    "    # Remove words that are only one character.\n",
    "    docs0 = [[token for token in doc if len(token) > 1] for doc in docs0]\n",
    "    docs1 = [[token for token in doc if len(token) > 1] for doc in docs1]\n",
    "\n",
    "    # Lemmatize the documents. Better than stemmer as is easier to read\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs0 = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs0]\n",
    "    docs1 = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs1]\n",
    "\n",
    "    # Add bigrams to docs (only ones that appear 20 times or more).\n",
    "    bigram = Phrases(docs0, min_count=20)\n",
    "    for idx in range(len(docs0)):\n",
    "        for token in bigram[docs0[idx]]:\n",
    "            if '_' in token:\n",
    "                # Token is a bigram, add to document.\n",
    "                docs0[idx].append(token)\n",
    "    bigram = Phrases(docs1, min_count=20)\n",
    "    for idx in range(len(docs1)):\n",
    "        for token in bigram[docs1[idx]]:\n",
    "            if '_' in token:\n",
    "                # Token is a bigram, add to document.\n",
    "                docs1[idx].append(token)\n",
    "\n",
    "    # Remove rare and common tokens.\n",
    "    # Create a dictionary representation of the documents.\n",
    "    dic0 = Dictionary(docs0)\n",
    "    dic1 = Dictionary(docs1)\n",
    "\n",
    "    # Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "    dic0.filter_extremes(no_below=20, no_above=0.5)\n",
    "    dic1.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "    if union:\n",
    "        transformer = dic0.merge_with(dic1)\n",
    "    else:\n",
    "        good_ids0 = []\n",
    "        good_ids1 = []\n",
    "        for good_value in set(dic0.values()).intersection(set(dic1.values())):\n",
    "            good_ids0.append(dic0.token2id[good_value])\n",
    "            good_ids1.append(dic1.token2id[good_value])\n",
    "        dic0.filter_tokens(good_ids=good_ids0)\n",
    "\n",
    "    dic1 = dic0\n",
    "\n",
    "    # Bag-of-words representation of the documents.\n",
    "    cor0 = [dic0.doc2bow(doc) for doc in docs0]\n",
    "    cor1 = [dic1.doc2bow(doc) for doc in docs1]\n",
    "    print('Number of unique tokens in dic0: %d' % len(dic0))\n",
    "    print('Number of unique tokens in dic1: %d' % len(dic1))\n",
    "    print('Number of documents of cor0: %d' % len(cor0))\n",
    "    print('Number of documents of cor1: %d' % len(cor1))\n",
    "\n",
    "    return dic0, cor0, dic1, cor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_lda(dictionary, corpus):\n",
    "    # Train LDA model.\n",
    "\n",
    "    # Set training parameters.\n",
    "    num_topics = 4\n",
    "    chunksize = 100000\n",
    "    passes = 20\n",
    "    iterations = 300\n",
    "    eval_every = None  # Evaluating model perplexity takes a lot of time.\n",
    "\n",
    "    # Make an index to word dictionary.\n",
    "    temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "    id2word = dictionary.id2token\n",
    "\n",
    "    model = LdaMulticore(\n",
    "        corpus=corpus,\n",
    "        num_topics=num_topics,\n",
    "        id2word=id2word,\n",
    "        workers=4,\n",
    "        chunksize=chunksize,\n",
    "        passes=passes,\n",
    "        alpha='symmetric',\n",
    "        eta='auto',\n",
    "        eval_every=eval_every,\n",
    "        iterations=iterations,\n",
    "    )\n",
    "\n",
    "    top_topics = model.top_topics(corpus)\n",
    "\n",
    "    # Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "    avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "    print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "    pprint(top_topics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data_train_1 = load_wikitext()\n",
    "data_train_2 = load_arxiv()\n",
    "dictionary_1, corpus_1, dictionary_2, corpus_2 = tokenize_special(data_train_1, data_train_2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ldamodel_1 = train_lda(dictionary_1, corpus_1)\n",
    "ldamodel_2 = train_lda(dictionary_2, corpus_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load pre-trained LDA Models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "path1 = \"lda-wiki_nt-gpt2_nt/wiki_nt\"\n",
    "path2 = \"lda-wiki_nt-gpt2_nt/gpt2_nt\"\n",
    "topics = 5\n",
    "mode = \"intersection\"   # \"union\"\n",
    "\n",
    "path_ldamodel_1 = f\"data/{path1}/{mode}/{topics}/ldamodel_{topics}\"\n",
    "path_ldamodel_2 = f\"data/{path2}/{mode}/{topics}/ldamodel_{topics}\"\n",
    "path_dictionary_1 = f\"data/{path1}/{mode}/{topics}/dictionary_{topics}\"\n",
    "path_dictionary_2 = f\"data/{path2}/{mode}/{topics}/dictionary_{topics}\"\n",
    "path_corpus_1 = f\"data/{path1}/{mode}/{topics}/corpus_{topics}\"\n",
    "path_corpus_2 = f\"data/{path2}/{mode}/{topics}/corpus_{topics}\"\n",
    "\n",
    "# Load a potentially pretrained model from disk.\n",
    "with open(path_corpus_1, 'r') as file:\n",
    "    corpus_1 = json.load(file)\n",
    "with open(path_corpus_2, 'r') as file:\n",
    "    corpus_2 = json.load(file)\n",
    "dictionary_1 = SaveLoad.load(path_dictionary_1)\n",
    "dictionary_2 = SaveLoad.load(path_dictionary_2)\n",
    "ldamodel_1 = LdaMulticore.load(path_ldamodel_1)\n",
    "ldamodel_2 = LdaMulticore.load(path_ldamodel_2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Visualizing an LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "vis_data_wiki = gensimvis.prepare(ldamodel_1, corpus_1, dictionary_1, sort_topics=False)\n",
    "pyLDAvis.display(vis_data_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "vis_data_gpt = gensimvis.prepare(ldamodel_2, corpus_2, dictionary_2, sort_topics=False)\n",
    "pyLDAvis.display(vis_data_gpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Comparing two LDA Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_difference_matplotlib(mdiff, title=\"\", annotation=None):\n",
    "    \"\"\"Helper function to plot difference between models.\n",
    "\n",
    "    Uses matplotlib as the backend.\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(figsize=(18, 14))\n",
    "    data = ax.imshow(mdiff, cmap='RdBu_r', vmin=0.0, vmax=1.0, origin='lower')\n",
    "    for axis in [ax.xaxis, ax.yaxis]:\n",
    "        axis.set_major_locator(MaxNLocator(integer=True))\n",
    "    plt.title(title)\n",
    "    plt.colorbar(data)\n",
    "    for (i, j) in [(0,1), (2,2)]:\n",
    "        ax.add_patch(Rectangle((j-.5, i-.5), 1, 1, 0.0, fill=None, ec='black', lw=50/topics))\n",
    "    #plt.savefig(f\"./data/test_{topics}.png\", dpi=300)\n",
    "\n",
    "distance = 'jensen_shannon' # 'kullback_leibler', 'hellinger', 'jaccard', 'jensen_shannon'\n",
    "num_words = 1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "mdiff, annotation = ldamodel_2.diff(ldamodel_1, distance=distance, num_words=num_words)\n",
    "plot_difference_matplotlib(mdiff, title=f\"Topic difference (two models)[{distance} distance]\", annotation=annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mdiff, annotation = ldamodel_1.diff(ldamodel_2, distance=distance, num_words=num_words)\n",
    "plot_difference_matplotlib(mdiff, title=f\"Topic difference (two models)[{distance} distance]\", annotation=annotation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Score function for evaluating how equal two LDA Models are"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def score_by_topic_probability(ldamodel_1, ldamodel_2, corpus_1, corpus_2, distance='jensen_shannon'):\n",
    "    mdiff1, annotation1 = ldamodel_1.diff(ldamodel_2, distance=distance, num_words=1000000000)\n",
    "    mdiff2, annotation2 = ldamodel_2.diff(ldamodel_1, distance=distance, num_words=1000000000)\n",
    "    min1 = np.amin(mdiff1, axis=1)\n",
    "    min2 = np.amin(mdiff2, axis=1)\n",
    "    topic_corpus_prob_1 = np.zeros(ldamodel_1.num_topics)\n",
    "    topic_corpus_prob_2 = np.zeros(ldamodel_2.num_topics)\n",
    "    probas_1 = ldamodel_1.get_document_topics(list(itertools.chain.from_iterable(corpus_1)), minimum_probability=0.0)\n",
    "    probas_2 = ldamodel_2.get_document_topics(list(itertools.chain.from_iterable(corpus_2)), minimum_probability=0.0)\n",
    "    for key, val in probas_1:\n",
    "        topic_corpus_prob_1[key] = val\n",
    "    for key, val in probas_2:\n",
    "        topic_corpus_prob_2[key] = val\n",
    "    return (np.sum(topic_corpus_prob_1 * min1) + np.sum(topic_corpus_prob_2 * min2))/2\n",
    "\n",
    "\n",
    "def score_by_top_topic(ldamodel_1, ldamodel_2, corpus_1, corpus_2, distance='jensen_shannon'):\n",
    "    mdiff1, annotation1 = ldamodel_1.diff(ldamodel_2, distance=distance, num_words=1000000000)\n",
    "    mdiff2, annotation2 = ldamodel_2.diff(ldamodel_1, distance=distance, num_words=1000000000)\n",
    "    min1 = np.amin(mdiff1, axis=1)\n",
    "    min2 = np.amin(mdiff2, axis=1)\n",
    "    cnt1 = np.zeros(ldamodel_1.num_topics)\n",
    "    for doc in corpus_1:\n",
    "        prob = ldamodel_1.get_document_topics(doc, minimum_probability=0.0)\n",
    "        topic = max(prob, prob.get)\n",
    "        cnt1[topic] += 1\n",
    "    cnt2 = np.zeros(ldamodel_1.num_topics)\n",
    "    for doc in corpus_2:\n",
    "        prob = ldamodel_2.get_document_topics(doc, minimum_probability=0.0)\n",
    "        topic = max(prob, prob.get)\n",
    "        cnt2[topic] += 1\n",
    "\n",
    "    return (np.sum(cnt1 * min1)/np.sum(cnt1) + np.sum(cnt2 * min2)/np.sum(cnt2)) / 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "model_pair = (\"lda-wiki_nt-gpt2_nt/wiki_nt\", \"lda-wiki_nt-gpt2_nt/gpt2_nt\")\n",
    "topic = 5\n",
    "mode = \"union\"\n",
    "path1 = model_pair[0]\n",
    "path2 = model_pair[1]\n",
    "path_ldamodel_1 = f\"data/{path1}/{mode}/{topic}/ldamodel_{topic}\"\n",
    "path_ldamodel_2 = f\"data/{path2}/{mode}/{topic}/ldamodel_{topic}\"\n",
    "path_dictionary_1 = f\"data/{path1}/{mode}/{topic}/dictionary_{topic}\"\n",
    "path_dictionary_2 = f\"data/{path2}/{mode}/{topic}/dictionary_{topic}\"\n",
    "path_corpus_1 = f\"data/{path1}/{mode}/{topic}/corpus_{topic}\"\n",
    "path_corpus_2 = f\"data/{path2}/{mode}/{topic}/corpus_{topic}\"\n",
    "\n",
    "# Load pretrained models from disk.\n",
    "with open(path_corpus_1, 'r') as file:\n",
    "    corpus_1 = json.load(file)\n",
    "with open(path_corpus_2, 'r') as file:\n",
    "    corpus_2 = json.load(file)\n",
    "dictionary_1 = SaveLoad.load(path_dictionary_1)\n",
    "dictionary_2 = SaveLoad.load(path_dictionary_2)\n",
    "ldamodel_1 = LdaMulticore.load(path_ldamodel_1)\n",
    "ldamodel_2 = LdaMulticore.load(path_ldamodel_2)\n",
    "\n",
    "distance = 'jensen_shannon'\n",
    "words = 100000000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "a = ldamodel_1.get_document_topics(list(itertools.chain.from_iterable(corpus_1)), minimum_probability=0.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "b = ldamodel_2.get_document_topics(list(itertools.chain.from_iterable(corpus_2)), minimum_probability=0.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "tup = max(b, key=itemgetter(1))[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "int"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tup)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "[(0, 0.023148753),\n (1, 0.0028030737),\n (2, 0.016371729),\n (3, 0.0014072682),\n (4, 0.0022580405),\n (5, 0.009407448),\n (6, 0.007316898),\n (7, 0.00894459),\n (8, 0.011694404),\n (9, 0.008548111),\n (10, 0.006001778),\n (11, 0.028335033),\n (12, 0.002321526),\n (13, 0.004526338),\n (14, 0.014262614),\n (15, 0.010423615),\n (16, 0.01470411),\n (17, 0.0023821525),\n (18, 0.0020809616),\n (19, 0.012897339),\n (20, 0.010726887),\n (21, 0.0061916686),\n (22, 0.0103839),\n (23, 0.007435687),\n (24, 0.0031629896),\n (25, 0.016836846),\n (26, 0.002798819),\n (27, 0.0020177981),\n (28, 0.011548738),\n (29, 0.0084394),\n (30, 0.011539556),\n (31, 0.0065206145),\n (32, 0.0028019636),\n (33, 0.009506046),\n (34, 0.009114019),\n (35, 0.00586084),\n (36, 0.0104628),\n (37, 0.0050772345),\n (38, 0.003920392),\n (39, 0.0066951495),\n (40, 0.016898306),\n (41, 0.020711036),\n (42, 0.008364165),\n (43, 0.026002608),\n (44, 0.005680909),\n (45, 0.00496998),\n (46, 0.009889519),\n (47, 0.006774921),\n (48, 0.0038970679),\n (49, 0.006484387),\n (50, 0.027740307),\n (51, 0.014414606),\n (52, 0.0048178937),\n (53, 0.0013946799),\n (54, 0.014102748),\n (55, 0.01654683),\n (56, 0.029850544),\n (57, 0.01273114),\n (58, 0.024326751),\n (59, 0.00998628),\n (60, 0.0033111721),\n (61, 0.0026842013),\n (62, 0.004044702),\n (63, 0.012609935),\n (64, 0.006484603),\n (65, 0.026749615),\n (66, 0.0029726266),\n (67, 0.0062902104),\n (68, 0.014521028),\n (69, 0.0019182445),\n (70, 0.0019713046),\n (71, 0.0063996883),\n (72, 0.01757745),\n (73, 0.005192617),\n (74, 0.0071957447),\n (75, 0.014796366),\n (76, 0.005896411),\n (77, 0.0026384585),\n (78, 0.007550672),\n (79, 0.013334626),\n (80, 0.011854834),\n (81, 0.021811888),\n (82, 0.015383605),\n (83, 0.0056268526),\n (84, 0.0137619125),\n (85, 0.0084514525),\n (86, 0.008424602),\n (87, 0.009235152),\n (88, 0.017063877),\n (89, 0.022567343),\n (90, 0.0076382826),\n (91, 0.008954756),\n (92, 0.010472317),\n (93, 0.004415355),\n (94, 0.009582832),\n (95, 0.0037587003),\n (96, 0.008193903),\n (97, 0.0084100915),\n (98, 0.032025684),\n (99, 0.0017650278)]"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Full Data Processing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a score-topics graph for all model types"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def calc_score():\n",
    "    topics = np.asarray([2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 50, 100])\n",
    "    modes = [\"intersection\", \"union\"]\n",
    "    model_pairs = [\n",
    "        (\n",
    "            \"lda-wiki_nt-gpt2_nt/wiki_nt\",\n",
    "            \"lda-wiki_nt-gpt2_nt/gpt2_nt\"\n",
    "        ),\n",
    "        (\n",
    "            \"lda-gpt2_nt-gpt2_nt/gpt2_1_nt\",\n",
    "            \"lda-gpt2_nt-gpt2_nt/gpt2_2_nt\"\n",
    "        ),\n",
    "        (\n",
    "            \"lda-gpt2_nt-arxiv/gpt2_nt\",\n",
    "            \"lda-gpt2_nt-arxiv/arxiv\"\n",
    "        ),\n",
    "        (\n",
    "            \"lda-wiki_nt-arxiv/wiki_nt\",\n",
    "            \"lda-wiki_nt-arxiv/arxiv\"\n",
    "        ),\n",
    "        (\n",
    "            \"lda-gpt2-gpt2/gpt2_1\",\n",
    "            \"lda-gpt2-gpt2/gpt2_2\",\n",
    "        )\n",
    "    ]\n",
    "    length = len(topics) * len(modes) * len(model_pairs)\n",
    "    with tqdm(total=length) as pbar:\n",
    "        for model_pair in model_pairs:\n",
    "            for mode in modes:\n",
    "                for idx, topic in enumerate(topics):\n",
    "                    path1 = model_pair[0]\n",
    "                    path2 = model_pair[1]\n",
    "                    path_ldamodel_1 = f\"data/{path1}/{mode}/{topic}/ldamodel_{topic}\"\n",
    "                    path_ldamodel_2 = f\"data/{path2}/{mode}/{topic}/ldamodel_{topic}\"\n",
    "                    path_dictionary_1 = f\"data/{path1}/{mode}/{topic}/dictionary_{topic}\"\n",
    "                    path_dictionary_2 = f\"data/{path2}/{mode}/{topic}/dictionary_{topic}\"\n",
    "                    path_corpus_1 = f\"data/{path1}/{mode}/{topic}/corpus_{topic}\"\n",
    "                    path_corpus_2 = f\"data/{path2}/{mode}/{topic}/corpus_{topic}\"\n",
    "\n",
    "                    # Load pretrained models from disk.\n",
    "                    with open(path_corpus_1, 'r') as file:\n",
    "                        corpus_1 = json.load(file)\n",
    "                    with open(path_corpus_2, 'r') as file:\n",
    "                        corpus_2 = json.load(file)\n",
    "                    dictionary_1 = SaveLoad.load(path_dictionary_1)\n",
    "                    dictionary_2 = SaveLoad.load(path_dictionary_2)\n",
    "                    ldamodel_1 = LdaMulticore.load(path_ldamodel_1)\n",
    "                    ldamodel_2 = LdaMulticore.load(path_ldamodel_2)\n",
    "\n",
    "                    distance = 'jensen_shannon'\n",
    "                    words = 100000000\n",
    "                    short_mode = \"is\" if mode == \"intersection\" else \"un\"\n",
    "                    '''\n",
    "                    # Compare models with score and save\n",
    "                    diff_score = score_by_top_topic(ldamodel_1, ldamodel_2, corpus_1, corpus_2)\n",
    "\n",
    "                    score_topic_graph_file = \"data/score_topic_graph_values.json\"\n",
    "\n",
    "                    if os.path.isfile(score_topic_graph_file):\n",
    "                        with open(score_topic_graph_file, 'r') as file:\n",
    "                            score_topic_graph_values = json.load(file)\n",
    "                    else:\n",
    "                        score_topic_graph_values = dict()\n",
    "\n",
    "\n",
    "                    key = f\"{path1.split('/')[0]}-{short_mode}\"\n",
    "                    if key not in score_topic_graph_values.keys():\n",
    "                        score_topic_graph_values[key] = np.ones(topics.shape).tolist()\n",
    "\n",
    "                    score_topic_graph_values[key][idx] = diff_score\n",
    "\n",
    "                    with open(score_topic_graph_file, 'w') as file:\n",
    "                        json.dump(score_topic_graph_values, file)\n",
    "                    '''\n",
    "                    # Calculate Difference Graph and save it\n",
    "                    mdiff, annotation = ldamodel_1.diff(ldamodel_2, distance=distance, num_words=1000000000)\n",
    "                    fig, ax = plt.subplots(figsize=(18, 14))\n",
    "                    data = ax.imshow(mdiff, cmap='RdBu_r', vmin=0.0, vmax=1.0, origin='lower')\n",
    "                    for axis in [ax.xaxis, ax.yaxis]:\n",
    "                        axis.set_major_locator(MaxNLocator(integer=True))\n",
    "                    plt.title(\n",
    "                        f\"Topic difference ({path1.split('/')[1]} - {path2.split('/')[1]} - {mode})[{distance} distance] for {topic} topics\")\n",
    "                    plt.colorbar(data)\n",
    "                    plt.savefig(f\"./data/{path1.split('/')[0]}/diff_{short_mode}_{topic}.png\", dpi=300)\n",
    "                    plt.close('all')\n",
    "                    pbar.update(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/120 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6af1d9d6a8834ef292721e891dbed628"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_4984/1503644745.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mcalc_score\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_4984/4103467662.py\u001B[0m in \u001B[0;36mcalc_score\u001B[1;34m()\u001B[0m\n\u001B[0;32m     42\u001B[0m                         \u001B[0mcorpus_1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mjson\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfile\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     43\u001B[0m                     \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath_corpus_2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'r'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mfile\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 44\u001B[1;33m                         \u001B[0mcorpus_2\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mjson\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfile\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     45\u001B[0m                     \u001B[0mdictionary_1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSaveLoad\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath_dictionary_1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     46\u001B[0m                     \u001B[0mdictionary_2\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSaveLoad\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath_dictionary_2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\json\\__init__.py\u001B[0m in \u001B[0;36mload\u001B[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[0m\n\u001B[0;32m    291\u001B[0m     \u001B[0mkwarg\u001B[0m\u001B[1;33m;\u001B[0m \u001B[0motherwise\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mJSONDecoder\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;31m`\u001B[0m \u001B[1;32mis\u001B[0m \u001B[0mused\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    292\u001B[0m     \"\"\"\n\u001B[1;32m--> 293\u001B[1;33m     return loads(fp.read(),\n\u001B[0m\u001B[0;32m    294\u001B[0m         \u001B[0mcls\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcls\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mobject_hook\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mobject_hook\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    295\u001B[0m         \u001B[0mparse_float\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mparse_float\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mparse_int\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mparse_int\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\json\\__init__.py\u001B[0m in \u001B[0;36mloads\u001B[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[0m\n\u001B[0;32m    344\u001B[0m             \u001B[0mparse_int\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mparse_float\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mand\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    345\u001B[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001B[1;32m--> 346\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0m_default_decoder\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdecode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    347\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mcls\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    348\u001B[0m         \u001B[0mcls\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mJSONDecoder\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\json\\decoder.py\u001B[0m in \u001B[0;36mdecode\u001B[1;34m(self, s, _w)\u001B[0m\n\u001B[0;32m    335\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    336\u001B[0m         \"\"\"\n\u001B[1;32m--> 337\u001B[1;33m         \u001B[0mobj\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mend\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mraw_decode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0midx\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0m_w\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    338\u001B[0m         \u001B[0mend\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_w\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mend\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    339\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mend\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\json\\decoder.py\u001B[0m in \u001B[0;36mraw_decode\u001B[1;34m(self, s, idx)\u001B[0m\n\u001B[0;32m    351\u001B[0m         \"\"\"\n\u001B[0;32m    352\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 353\u001B[1;33m             \u001B[0mobj\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mend\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mscan_once\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0midx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    354\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mStopIteration\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    355\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mJSONDecodeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Expecting value\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0ms\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0merr\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "calc_score()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "score_topic_graph_file = \"data/score_topic_graph_values.json\"\n",
    "\n",
    "if os.path.isfile(score_topic_graph_file):\n",
    "    with open(score_topic_graph_file, 'r') as file:\n",
    "        score_topic_graph_values = json.load(file)\n",
    "\n",
    "names = score_topic_graph_values.values()\n",
    "values = score_topic_graph_values.values()\n",
    "plt.clf()\n",
    "fig, axes = plt.subplots()\n",
    "for idx, value in enumerate(values):\n",
    "    axes.plot(topics, value, label=names[idx])\n",
    "axes.legend()\n",
    "axes.set_title(f\"Score-Topic Graph for LDA Models\")\n",
    "axes.set_xscale('log')\n",
    "axes.set_xlabel('Number of Topics')\n",
    "axes.set_ylabel('Score (lower is better)')\n",
    "axes.set_xticks(topics)\n",
    "axes.get_xaxis().set_major_formatter(ScalarFormatter())\n",
    "plt.savefig(f\"./data/score_topic_graph.png\", dpi=300)\n",
    "plt.close('all')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "switch = True\n",
    "if switch:\n",
    "    score_file_path = \"data/score_by_top_topic.json\"\n",
    "    title = \"'Score by Top Topic'-Topic Graph for LDA Models\"\n",
    "    y_label = \"Score by Top Topic (lower is better)\"\n",
    "else:\n",
    "    score_file_path = \"data/score_by_topic_probability_values.json\"\n",
    "    title = \"'Score by Topic Probability'-Topic Graph for LDA Models\"\n",
    "    y_label = \"Score by Topic Probability (lower is better)\"\n",
    "if os.path.isfile(score_file_path):\n",
    "    with open(score_file_path, 'r') as file:\n",
    "        score_values = json.load(file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "['lda-wiki_nt-gpt2_nt-is',\n 'lda-wiki_nt-gpt2_nt-un',\n 'lda-gpt2_nt-gpt2_nt-is',\n 'lda-gpt2_nt-gpt2_nt-un',\n 'lda-gpt2_nt-arxiv-is',\n 'lda-gpt2_nt-arxiv-un',\n 'lda-wiki_nt-arxiv-is',\n 'lda-wiki_nt-arxiv-un',\n 'lda-gpt2-gpt2-is',\n 'lda-gpt2-gpt2-un']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(score_values.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a Wordcloud of every topic in an LDA Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = np.array([2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "for model in tqdm([0, 1, 2], desc='Model', leave=True):\n",
    "    for topics in tqdm(x, desc='Topic', leave=False):\n",
    "        if model == 0:\n",
    "            path1 = \"lda-wiki_no_titles-gpt2_no_titles/wiki_nt\"\n",
    "            path2 = \"lda-wiki_no_titles-gpt2_no_titles/gpt2_nt\"\n",
    "        elif model == 1:\n",
    "            path1 = \"lda-gpt2_no_titles-gpt2_no_titles/gpt2_1_nt\"\n",
    "            path2 = \"lda-gpt2_no_titles-gpt2_no_titles/gpt2_2_nt\"\n",
    "        else:\n",
    "            path1 = \"lda-gpt2-gpt2/gpt2_1\"\n",
    "            path2 = \"lda-gpt2-gpt2/gpt2_2\"\n",
    "\n",
    "        path_ldamodel_1 = f\"data/{path1}/{topics}/ldamodel_{topics}\"\n",
    "        path_ldamodel_2 = f\"data/{path2}/{topics}/ldamodel_{topics}\"\n",
    "        path_dictionary_1 = f\"data/{path1}/{topics}/dictionary_{topics}\"\n",
    "        path_dictionary_2 = f\"data/{path2}/{topics}/dictionary_{topics}\"\n",
    "        path_corpus_1 = f\"data/{path1}/{topics}/corpus_{topics}\"\n",
    "        path_corpus_2 = f\"data/{path2}/{topics}/corpus_{topics}\"\n",
    "\n",
    "        # Load a potentially pretrained model from disk.\n",
    "        with open(path_corpus_1, 'r') as file:\n",
    "            corpus_1 = json.load(file)\n",
    "        with open(path_corpus_2, 'r') as file:\n",
    "            corpus_2 = json.load(file)\n",
    "        dictionary_1 = SaveLoad.load(path_dictionary_1)\n",
    "        dictionary_2 = SaveLoad.load(path_dictionary_2)\n",
    "        ldamodel_1 = LdaMulticore.load(path_ldamodel_1)\n",
    "        ldamodel_2 = LdaMulticore.load(path_ldamodel_2)\n",
    "        for topic in range(ldamodel_1.num_topics):\n",
    "            plt.clf()\n",
    "            plt.figure()\n",
    "            plt.imshow(WordCloud().fit_words(dict(ldamodel_1.show_topic(topic, 200))))\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(f\"{path1.split('/')[1]} LDA Model with {topics} Topics - Topic {topic}\")\n",
    "            plt.savefig(f\"./data/{path1.split('/')[0]}/wordcloud_{path1.split('/')[1]}_{topics}_{topic}.png\", dpi=600)\n",
    "\n",
    "        for topic in range(ldamodel_2.num_topics):\n",
    "            plt.clf()\n",
    "            plt.figure()\n",
    "            plt.imshow(WordCloud().fit_words(dict(ldamodel_1.show_topic(topic, 200))))\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(f\"{path2.split('/')[1]} LDA Model with {topics} Topics - Topic {topic}\")\n",
    "            plt.savefig(f\"./data/{path2.split('/')[0]}/wordcloud_{path2.split('/')[1]}_{topics}_{topic}.png\", dpi=600)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DEAD ATTEMPTS (EVENTUALLY TO DELETE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Average Topic Coherence Graphs for all LDAs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for pathname in [\"data/lda-wiki_no_titles-gpt2_no_titles\", \"data/lda-gpt2_no_titles-gpt2_no_titles\", \"data/lda-gpt2-gpt2\"]:\n",
    "    path = pathname\n",
    "    x = np.array([2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 50, 100])\n",
    "    y_1 = np.zeros(12)\n",
    "    y_2 = np.zeros(12)\n",
    "    label1 = None\n",
    "    label2 = None\n",
    "\n",
    "    for filename in os.listdir(path + \"/execution_dump\"):\n",
    "        with open(os.path.join(path + \"/execution_dump\", filename), 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            rex = \"Average topic coherence: \"\n",
    "            idx = text.find(rex)\n",
    "            num = float(text[idx+len(rex):idx+len(rex)+7])\n",
    "            filenum = filename.split(\"_\")[-1][:-4]\n",
    "            if 'wiki_' in filename or 'gpt2_1_' in filename or 'gpt2_nt_1_' in filename:\n",
    "                y_1[np.where(x == int(filenum))] = num\n",
    "                label1 = '_'.join(filename.split(\"_\")[:-1])\n",
    "                print(\"ONE\")\n",
    "            else:\n",
    "                y_2[np.where(x == int(filenum))] = num\n",
    "                label2 = '_'.join(filename.split(\"_\")[:-1])\n",
    "                print(\"TWO\")\n",
    "            f.close()\n",
    "\n",
    "    plt.clf()\n",
    "    fig, axes = plt.subplots()\n",
    "    line1, = axes.plot(x, y_1, label=label1)\n",
    "    line2, = axes.plot(x, y_2, label=label2)\n",
    "    axes.legend(handles=[line1, line2])\n",
    "    axes.set_title(f\"Average Topic Coherence Graph\")\n",
    "    axes.set_xscale('log')\n",
    "    axes.set_xlabel('Number of Topics')\n",
    "    axes.set_ylabel('Umass Score')\n",
    "    axes.set_xticks(x)\n",
    "    axes.get_xaxis().set_major_formatter(ScalarFormatter())\n",
    "    plt.savefig(path+f\"/umass_plot_{label1}-{label2}.png\", dpi=300)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}